{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408aca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptanu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# step 0.1 Face Swap Classifier Training Notebook with Frame Extraction\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71642d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 face-cropped frames from 917_924.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 918_934.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 919_015.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 920_811.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 28 face-cropped frames from 923_023.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 924_917.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 925_933.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 927_912.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 928_160.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 929_962.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 932_384.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 933_925.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 934_918.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 935_733.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 938_987.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 939_115.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 942_943.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 943_942.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 944_032.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 945_044.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 946_049.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 947_951.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 948_965.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 949_868.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 950_836.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 951_947.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 952_882.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 953_974.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 954_976.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 956_958.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 957_959.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 958_956.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 959_957.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 960_999.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 961_069.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 962_929.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 965_948.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 966_988.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 968_884.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 969_897.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 971_564.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 972_718.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 974_953.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 976_954.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 977_075.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 980_992.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 981_985.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 982_004.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 983_113.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 985_981.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 986_994.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 987_938.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 988_966.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 989_993.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 990_008.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 991_064.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 992_980.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 993_989.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 994_986.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 996_056.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 998_561.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 999_960.mp4 to data/data_faces\\test_combined\\fake\n",
      "Saved 100 face-cropped frames from 917.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 918.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 919.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 920.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 30 face-cropped frames from 923.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 924.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 925.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 927.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 928.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 929.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 932.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 933.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 934.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 935.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 938.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 939.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 942.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 943.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 944.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 945.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 946.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 947.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 948.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 949.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 950.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 951.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 952.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 953.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 954.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 956.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 957.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 958.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 959.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 960.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 961.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 962.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 965.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 966.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 968.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 969.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 971.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 972.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 974.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 976.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 977.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 980.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 981.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 982.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 983.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 985.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 87 face-cropped frames from 986.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 987.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 988.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 989.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 97 face-cropped frames from 990.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 991.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 992.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 993.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 994.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 996.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 998.mp4 to data/data_faces\\test_combined\\real\n",
      "Saved 100 face-cropped frames from 999.mp4 to data/data_faces\\test_combined\\real\n"
     ]
    }
   ],
   "source": [
    "# step 0.2\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def extract_frames_from_videos(video_folder, output_folder, split, class_name, frames_per_video=100):\n",
    "    \"\"\"\n",
    "    Extracts at least `frames_per_video` face-cropped frames from each video in `video_folder` \n",
    "    into `output_folder/split/class_name` using Mediapipe for face detection.\n",
    "    Crops the face region, draws a bounding box, resizes to 112x112, and saves it.\n",
    "    \"\"\"\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "    # Set the correct path based on split and class_name\n",
    "    output_class_dir = os.path.join(output_folder, split, class_name)\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "    for video_name in os.listdir(video_folder):\n",
    "        if not video_name.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_folder, video_name)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        interval = max(1, total_frames // frames_per_video)\n",
    "\n",
    "        count = 0\n",
    "        saved = 0\n",
    "\n",
    "        while cap.isOpened() and saved < frames_per_video:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if count % interval == 0:\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_detection.process(rgb_frame)\n",
    "\n",
    "                if results.detections:\n",
    "                    detection = results.detections[0]\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    h, w, _ = frame.shape\n",
    "                    x1 = int(bboxC.xmin * w)\n",
    "                    y1 = int(bboxC.ymin * h)\n",
    "                    x2 = int((bboxC.xmin + bboxC.width) * w)\n",
    "                    y2 = int((bboxC.ymin + bboxC.height) * h)\n",
    "\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "                    face_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                    if face_crop.size == 0:\n",
    "                        count += 1\n",
    "                        continue\n",
    "\n",
    "                    cv2.rectangle(face_crop, (0, 0), (x2 - x1, y2 - y1), (0, 255, 0), 2)\n",
    "                    face_resized = cv2.resize(face_crop, (112, 112))\n",
    "                    frame_filename = f\"{os.path.splitext(video_name)[0]}_face{count}.jpg\"\n",
    "                    frame_path = os.path.join(output_class_dir, frame_filename)\n",
    "                    \n",
    "                    if os.path.exists(frame_path):\n",
    "                        count += 1\n",
    "                        continue\n",
    "\n",
    "                    cv2.imwrite(frame_path, face_resized)\n",
    "                    saved += 1\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Saved {saved} face-cropped frames from {video_name} to {output_class_dir}\")\n",
    "\n",
    "    face_detection.close()\n",
    "\n",
    "\n",
    "# --------- CALLING CODE ---------\n",
    "# Map the video folders to the correct output folders\n",
    "\n",
    "'''\n",
    "extract_frames_from_videos(\"data/train/fake\", \"data/data_faces\", \"train_combined\", \"fake\", frames_per_video=100)\n",
    "extract_frames_from_videos(\"data/train/real\", \"data/data_faces\", \"train_combined\", \"real\", frames_per_video=100)\n",
    "extract_frames_from_videos(\"data/val/fake\", \"data/data_faces\", \"val_combined\", \"fake\", frames_per_video=100)\n",
    "extract_frames_from_videos(\"data/val/real\", \"data/data_faces\", \"val_combined\", \"real\", frames_per_video=100)\n",
    "'''\n",
    "# now extracting from test set\n",
    "\n",
    "extract_frames_from_videos(\"data/test/fake\", \"data/data_faces\", \"test_combined\", \"fake\", frames_per_video=100)\n",
    "extract_frames_from_videos(\"data/test/real\", \"data/data_faces\", \"test_combined\", \"real\", frames_per_video=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- STEP 1: Data transforms ----------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 2: Load datasets ----------\n",
    "data_faces_dir = \"./data/data_faces\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=os.path.join(data_faces_dir, \"train_combined\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=os.path.join(data_faces_dir, \"val_combined\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32029d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After defining train_loader:\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(\"Batch index:\", i)\n",
    "    print(\"Input batch shape:\", inputs.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    print(\"Sample labels:\", labels)\n",
    "\n",
    "    # You can even visualize a few of the images in the batch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize (if you normalized)\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "    if i == 0: # Only inspect the first batch\n",
    "        imshow(inputs[0].cpu()) # Show the first image in the batch\n",
    "        print(\"Label of the first image:\", labels[0])\n",
    "        break # Stop after inspecting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc64845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 3: Model Definition ----------\n",
    "class FaceSwapDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceSwapDetector, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.model(x))\n",
    "\n",
    "model = FaceSwapDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- STEP 4: Loss, optimizer, and device ----------\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- STEP 5: Training loop ----------\n",
    "\n",
    "'''\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    # calcualte the training accuracy also along with training loss and validation accuracy\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []   # <-- Add this list\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # --- Calculate training accuracy ---\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_train_acc = 100 * correct / total   # Training accuracy for this epoch\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_train_acc)   # Store it!\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_train_acc:.2f}%\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eef404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- STEP 6: Plot validation accuracy ----------\n",
    "#plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='Train Accuracy', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71850cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 7:only train loss Plot ----------\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d556482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Combined Train and Validation Accuracy Plot ----------\n",
    "\n",
    "plt.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"face_swap_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"full_face_swap_model.pth\") # save full model + architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you saved only state_dict\n",
    "model = YourModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(\"face_swap_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# OR, if you saved the full model\n",
    "model = torch.load(\"full_face_swap_model.pth\")\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
